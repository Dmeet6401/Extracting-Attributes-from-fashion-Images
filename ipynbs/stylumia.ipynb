{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DanDF29lWYL",
        "outputId": "5b0e620f-cc40-448c-e2c2-00aafac04037"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def unzip_file(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Example usage\n",
        "zip_path = '/content/drive/MyDrive/tasks/stylumia/train.zip'  # Specify the path to your zip file\n",
        "extract_path = '/content/drive/MyDrive/tasks/stylumia/'  # Specify the path where you want to extract the contents\n",
        "\n",
        "unzip_file(zip_path, extract_path)\n"
      ],
      "metadata": {
        "id": "9LPOVX5_gsqA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "zip_path = '/content/drive/MyDrive/tasks/stylumia/val.zip'  # Specify the path to your zip file\n",
        "extract_path = '/content/drive/MyDrive/tasks/stylumia/'  # Specify the path where you want to extract the contents\n",
        "\n",
        "unzip_file(zip_path, extract_path)"
      ],
      "metadata": {
        "id": "9Pc_BUzNiKj9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZoAjWp8dkDY",
        "outputId": "a3625f8a-b6ae-447b-8ce2-23545e1ce99e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of files: 3679\n",
            "Total number of files: 14712\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_files(folder_path):\n",
        "    total_files = 0\n",
        "    \n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        total_files += len(files)\n",
        "    \n",
        "    return total_files\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/tasks/stylumia/val'  # Replace with the actual path to your folder\n",
        "total_files = count_files(folder_path)\n",
        "print(f\"Total number of files: {total_files}\")\n",
        "folder_path = '/content/drive/MyDrive/tasks/stylumia/train'  # Replace with the actual path to your folder\n",
        "total_files = count_files(folder_path)\n",
        "print(f\"Total number of files: {total_files}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from keras import Sequential,models\n",
        "from keras.layers import Dense,Flatten,BatchNormalization,Conv2D,MaxPooling2D,MaxPool2D, Dropout, GlobalAveragePooling2D,Activation\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import itertools\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# %matplotlib inline"
      ],
      "metadata": {
        "id": "oQdBkK88ik1z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dir ='/content/drive/MyDrive/tasks/stylumia/train'\n",
        "val_dir = '/content/drive/MyDrive/tasks/stylumia/val'\n"
      ],
      "metadata": {
        "id": "S0iyWwl8jyXy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width=150\n",
        "img_height=150\n",
        "batch_size=128"
      ],
      "metadata": {
        "id": "my1wD5Oej7Gm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    # subset=\"training\",\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Aepm13j8SX",
        "outputId": "6f58a3a2-e20f-4476-e3f7-66e1ad78960a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14712 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(val_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    # subset=\"training\",\n",
        "                                                    class_mode='sparse',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7Obkjrj-bz",
        "outputId": "927d49cb-0fbb-4468-9117-c6ac70f0d037"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3679 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filepath=\"/content/drive/MyDrive/tasks/stylumia/VGG16_150-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "GBeK72R7j_zW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/MyDrive/tasks/stylumia/VGG16_150-50-0.67.hdf5')"
      ],
      "metadata": {
        "id": "GDT4pvRZkGVe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = loaded_model.fit(train_generator,epochs=100,validation_data=val_generator,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "id": "i09NE-jFkUrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a5d86f-2f2e-4fd7-823a-0743958b156a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "104/115 [==========================>...] - ETA: 4:31 - loss: 0.6482 - accuracy: 0.7405"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whsB4ZX0bfUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}